{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Machine learning basics and linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single day we generate over 2.5 quintillion bytes of data in all possible forms such as photos, videos or social media posts. To make sense of these amounts of data we rely on complicated algorithms that can efficiently store and analyze it. For some types of data it is however impossible to design a fixed algorithm that performs a certain task. Recognizing the content of an image using only the raw pixel values for example is a very complicated problem for which it is almost impossible to define a fixed sequence of steps that performs this task. This is were machine learning comes in. **Machine learning (ML)** is the scientific field that make computers learn from data. Instead of designing an algorithm that directly performs a certain task, we design algorithms that learn to perform this task from example data points. Machine learning is thus complimentary to traditional algorithms, there is no point in using a machine learning technique if an (efficient) algorithm is available for a certain task (e.g. sorting). Machine learning really shines when we can not easily describe exactly *how* something should be done but instead we can give many examples of our desired functionality. It is for example hard to come up with a set of rules that classifies an e-mail as spam or not but not to collect hundreds of examples of spam and normal e-mail.\n",
    "\n",
    "Machine learning is a subfield of **Artificial Intelligence (AI)** which comprises much broader techniques to build machines that demonstrate some kind of intelligence. Techniques such as planning algorithms and symbolic reasoning are part of AI but not of ML. Machine learning has many intersections with other fields such as **robotics**, **signal processing**, **statistics**, **optimization**, **data mining** (extracting useful data from large databases) or **pattern recognition** (automatic discovery of regularities in data ).\n",
    "\n",
    "Machine learning practitioners use statistical **models** that capture information from a given set of **training data** (**train set**). The hope is that the models learn the underlying properties without being overly sensitive to noise in the data. After training, the models are then used to make predictions about new, unseen data (**test set**). This stage is called **inference** and a good model should be able to **generalize** to data that is slightly different than the data that was seen during training. Machine learning models will almost never generalize perfectly in the way we hope they will, the goal is thus to reach an acceptable score on some kind of **performance metric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of machine learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of machine learning and machine learning models and depending on the task, the data or on prior knowledge about the problem, some models are better suited than others. \n",
    "- **Supervised learning** techniques require a labeled training set, i.e. a collection of examples (X), each annotated with a **label** (Y). In the case of image recognition this would be the type of object that is seen in the picture or in the case of spam detection this would be a boolean flag, indicating whether the e-mail is spam or not. Both of these examples require the model to predict a class that the input belongs to (cat or dog, spam or not spam). These types of problems where we are predicting a class membership are called **classification**. If we instead are tasked with predicting a real number (e.g. what will the stock price be tomorrow, based on the current market condition), then we are dealing with a **regression** problem.\n",
    "\n",
    "- **Unsupervised learning** techniques do not need a labeled training set and consequently will not learn a mapping to some predefined output. Instead, unsupervised learning algorithms try to find regularities in the input space that are a sign of some underlying structure to the data. Examples of unsupervised learning are **density estimation** (finding out what inputs occur more often than others), **clustering** (grouping similar data points into **clusters**) and **dimensionality reduction** (compressing the information into a lower dimensional representation that removes redundant parameters).\n",
    "\n",
    "- **Reinforcement learning** deals with problems where the model needs to interact directly with the environment. Examples could be a robot navigating within a building or a game AI playing against a human player in a video game. The task of the model (**agent**) is to come up with a sequence of actions (**policy**) that it needs to execute within the environment to end up in a desired state. The agent receives feedback (**reward**) after some actions and needs to update its policy to favor actions that result in higher rewards. The difficulty of these types of problems is that the reward is often **sparse** i.e. the agent only receives a reward at the end of a sequence (e.g. when the game is over). It then needs to figure out exactly which combination of actions have resulted in this reward and how this can be improved in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/workflow.png),<br/>Source: https://towardsdatascience.com/workflow-of-a-machine-learning-project-ec1dba419b94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The development of a machine learning model typically looks like the figure above. It all starts with collecting a dataset. This dataset is then split into a **Test set** and a **Train set**. The model is trained on the train set and after training, its generalization performance is evaluated on the hold-out test set. This is repeated until a good model configuration is found. The model is then released into production to make prediction for real users. Depending on the data and the problem there will be additional steps such as **Data preprocessing**,  **Data cleaning**, **Feature extraction**, **Feature selection**, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will predict the price of a house based on information such as the location and the number of rooms. This dataset (Boston Housing Dataset) is a well known public dataset that is often used in introductory machine learning courses. \n",
    "<div style=\"float:left\">\n",
    "        <img src=\"images/forsale.jpg\" width=\"550\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the the entire machine learning pipeline with this dataset and a very simple regression model: Linear regression. In all sessions we will intensively use different typical python data science tools such as:\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib (seaborn)\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "from matplotlib import rcParams\n",
    "\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 20,16\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is such a common dataset, the sklearn library has a utility method to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now loaded in a dictionary.\n",
    "<font color='red'>Task: explore the dataset object: print the keys of this dictionary and a few entries for each key</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following observations can be made:\n",
    "- There are 506 datapoints (houses)\n",
    "- For each of these we have 13 **features**. These are the characteristics that we can use to make our predictions. All the features are floating point numbers.\n",
    "- For each of these we have a single number. This is our target, the **ground-truth**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Transform this into a pandas dataframe for easier processing. The dataframe has as columns the different features, and as rows the data for each house. Add the target as an additional column. Description and file name are no longer needed. Use the head() and describe() methods to display some information about the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston = ...\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Visualize the distribution of the target variable</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Show the correlation matrix between the different features</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: What is the mathematical meaning of the number in each cell. What are the most interesting and least interesting features ? Do these make sense ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Plot two features (not the target) as a function of each other. Do this for two features with a very high  (positive) correlation, a very low (negative) correlation and a correlation around 0.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Plot the target as a function of each feature.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: What are the most interesting features ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test - Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the introduction, the goal of machine learning is to build models on train data that are able to make predictions on unseen test data. So we split our data into a **trainset** (80%) and a **test set** (20%). Based on the correlation matrix and the scatterplots, we select \"LSTAT\" and \"RM\" as the initial features of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.concat((boston['LSTAT'], boston['RM']), axis=1)\n",
    "Y = boston['TARGET']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that we can model the target using a **linear** combination of the input features. This means that we can approximate the target using a **weighted sum** of the input features.\n",
    "\n",
    "We can write the model as $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon = \\beta_0 + \\sum_{i=1}^{n} \\beta_i X_i + \\epsilon$\n",
    "\n",
    "where :\n",
    "- $Y$ is our target value\n",
    "- $\\beta_i$ are the **parameters** of the model that we need to find\n",
    "- $X_i$ are the values of our input features\n",
    "- $\\epsilon$ is an noise term that we are unable to model.\n",
    "\n",
    "In the simplest case with only one input feature, this reduces to $Y = \\beta_0 + \\beta_1 X_1 + \\epsilon$ or $Y = ax + b$, the equation of a straight line with slope $a$ and intercept $b$. $b$ is often called a **bias**. For convenience we incorporate this bias in the weight matrix $\\beta$ and extend the input features $X_i$ with a constant feature with value one.\n",
    "\n",
    "All that linear regression algorithms do is fit a straight line (**trend line**) through the datapoints and once we know the equation of the line, we can make predictions for every possible feature combination. If we have two input features we instead try to fit a plane in 3D space.\n",
    "\n",
    "<div>\n",
    "    <div style=\"float:left\">\n",
    "        <img src=\"images/linear_regression.png\" width=\"550\" />\n",
    "        <br/>Source: https://en.wikipedia.org/wiki/Linear_regression  \n",
    "    </div>\n",
    "    <div style=\"float:left, margin-left:600px\">\n",
    "        <img src=\"images/3d.jpg\" width=\"600\"/>\n",
    "        <br/>Source:https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to write this in matrix notation as $y = X\\beta + \\epsilon$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y$ is the set of target values (n points)\n",
    "- $X$ is the set of features values (p values for each of the n points)\n",
    "- $\\beta$ are the parameters of the model, (p weights, one for each input feature)\n",
    "- $\\epsilon$ is the noise for each datapoint (n values)\n",
    "\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & ... & x_{1p}\\\\\n",
    "1 & x_{21} & ... & x_{2p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & ... & x_{np}\n",
    "\\end{pmatrix} *\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_p\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "\\epsilon_1\\\\\n",
    "\\epsilon_2\\\\\n",
    "\\vdots\\\\\n",
    "\\epsilon_n\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We have now defined our problem, our dataset and our model. The next step is to use the model to capture information about the training data. This process is called **training** the model. It usually involves an iterative process that changes the **parameters** of the model. This is an **optimization** problem: find the values $\\beta$ that make the line go through the data as best as possible.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training.gif\" width=\"550\" />\n",
    "    <br/>Source: https://towardsdatascience.com/linear-regression-the-easier-way-6f941aa471ea\n",
    "</div>\n",
    "\n",
    "\n",
    "Each optimization problem needs a **loss function** to minimize. This loss function measures the **error** between our prediction and the actual target value. In this case we use **sum of squares** as our loss function: $\\sum_{i=1}^{n} (y - X\\beta)^2 = \\sum_{i=1}^{n} \\epsilon^2 $\n",
    "\n",
    "Since $\\epsilon$ is a vector, we can write $\\epsilon^2$ as $\\epsilon^\\intercal\\epsilon$ where $\\epsilon^\\intercal$ indicates the **transpose** of $\\epsilon$.\n",
    "\n",
    "$\\epsilon^\\intercal\\epsilon = \\begin{pmatrix} \\epsilon_1 & \\dots & \\epsilon_n \\end{pmatrix} * \\begin{pmatrix}\n",
    "\\epsilon_1\\\\\n",
    "\\vdots\\\\\n",
    "\\epsilon_n\n",
    "\\end{pmatrix} = \\epsilon_1^2 + \\dots + \\epsilon_n^2$\n",
    "\n",
    "Substituting $\\epsilon =  y - X\\beta$ this results in:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\epsilon^\\intercal\\epsilon &= (y - X\\beta)^\\intercal(y - X\\beta)\\\\\n",
    "&= (y^\\intercal - X^\\intercal\\beta^\\intercal)(y-X\\beta) \\\\\n",
    "&= y^\\intercal y - \\beta^\\intercal X^\\intercal y - y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X\\beta \\\\\n",
    "&= y^\\intercal y - 2 \\beta^\\intercal X^\\intercal y + \\beta^\\intercal X^\\intercal X\\beta\n",
    "\\end{align*}$\n",
    "<br/>\n",
    "Here we made use of the fact that $\\beta^\\intercal X^\\intercal y = (\\beta^\\intercal X^\\intercal y)^\\intercal = y^\\intercal X \\beta$ as they both represent the same scalar value and the transpose of a scalar is the value itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the $\\beta$ that minimizes this, we take the **partial derivative** of this function to the elements of $\\beta$. Finding the minimum is then equivalent to putting the partial derivatives to zero.\n",
    "\n",
    "<font color='red'>Task: What is the meaning of the partial derivative in this context ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\epsilon^\\intercal \\epsilon}{\\partial \\beta} = -2 X^\\intercal y + 2X^\\intercal X \\beta = 0$ (see https://medium.com/analytics-vidhya/multivariate-linear-regression-from-scratch-using-ols-ordinary-least-square-estimator-859646708cd6 for the derivation of this partial derivative).\n",
    "<br/>\n",
    "We now have an expression for $\\beta$ that gives us the smallest error:\n",
    "\n",
    "$ \\beta = (X^\\intercal X)^{-1} X^\\intercal y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: use this formula to calculate the optimal $\\beta$ from the datapoints in X_train and Y_train.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: What do each of these $\\beta$ values mean ? What does the sign mean ? What does the absolute value mean ? Do they make sense ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Implement a function *predict* that predicts the result for a new point.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "x = np.linspace(X_test[\"LSTAT\"].min(), X_test[\"LSTAT\"].max(), 30)\n",
    "y = np.linspace(X_test[\"RM\"].min(), X_test[\"RM\"].max(), 30)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "predictions = np.array([predict([b, x,y]) for b,x,y in zip(np.ones(len(np.ravel(X))), np.ravel(X), np.ravel(Y))])\n",
    "predictions = predictions.reshape(X.shape)\n",
    "\n",
    "ax.contour3D(X, Y, predictions, 50)\n",
    "\n",
    "ax.set_xlabel('LSTAT')\n",
    "ax.set_ylabel('RM')\n",
    "ax.set_zlabel('Prediction')\n",
    "\n",
    "ax.scatter3D(X_test[\"LSTAT\"], X_test[\"RM\"], Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: What would this plane look like if we forgot to add the bias ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performance of the model\n",
    "We now have a trained model for our data and we can visually see that it fits the data well. How can we quantify this into a single number in order to compare different methods ? For this we will use the **R-squared** metric. This is a statistical measure between 0 and 1 which calculates how similar a regression line is to the data it is fitted to. If it's a 1, the model is able to predict all the variance in the data, if it's a 0, the model predicts none of the variance. $R^2$ is often called the **coefficient of determination**. It is the proportion of the variance in the dependent variable that is predictable from the independent variable(s)\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{res}}{RR_{tot}}$\n",
    "\n",
    "With \n",
    "- $SS_{res}$: The sum of squares of residuals. $\\sum_{i=1}^{n} (y - X\\beta)^2 = \\sum_{i=1}^{n} \\epsilon_i^2 $\n",
    "- $SS_{tot}$: The total sum of squares (proportional to the variance of the data): $\\sum_{i=1}^{n} (y - \\bar{y})^2$  where $\\bar{y}$ indicates the mean of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Calculate the $R^2$ score for our model on the test set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/linear.png)<br/> XKCD Linear Regression: https://xkcd.com/1725/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is a relative **measure of fit**, it does not tell us anything about how close the predictions are to the real targets. To do this, we can instead use the **Root Mean Squared Error (RMSE)**. RMSE indicates how much our predication deviate from the target on average.\n",
    "\n",
    "$RMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y - X\\beta)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Calculate the RMSE score for our model on the test set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implemented a simple linear regression algorithm from scratch. When analyzing real data you should use existing implementations such as those from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# model evaluation for training set\n",
    "y_train_predict = lin_model.predict(X_train)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "r2 = r2_score(Y_train, y_train_predict)\n",
    "\n",
    "print(\"Model train performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# model evaluation for testing set\n",
    "y_test_predict = lin_model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "print(\"Model test performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Model parameters\")\n",
    "print(\"--------------------------------------\")\n",
    "print(lin_model.coef_)\n",
    "print(lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Refactor your code into a class that follows the same interface as the Sklearn LinearRegression class</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "     ...\n",
    "    \n",
    "lin_model = MyLinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "# model evaluation for training set\n",
    "y_train_predict = lin_model.predict(X_train)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "r2 = r2_score(Y_train, y_train_predict)\n",
    "\n",
    "print(\"Model train performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# model evaluation for testing set\n",
    "y_test_predict = lin_model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "print(\"Model test performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Model parameters\")\n",
    "print(\"--------------------------------------\")\n",
    "print(lin_model.coef_)\n",
    "print(lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: Train your model with different combinations of input features. What combination achieves the highest performance ?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression fits a linear function to your data. Not all data can be modeled by a linear relationship so in some cases it might be better to fit other functions such as exponentials or higher order polynomials. When designing the model we have to make assumptions on how our data can be modeled. These assumptions are called an **inductive bias**.\n",
    "- We now used a closed form solution to calculate the parameters of our model. This however relies on inverting and multiplying matrices. This will work fine for small matrices but is not efficient for large datasets. In practice, it is usually better to optimize these parameters with **gradient descent** (see one of the next sessions).\n",
    "- Because of the quadratic term in the loss function, linear regression is sensitive to **outliers** in the training set. A point at distance 10 from its prediction has 100 times the impact on the training error than a point at distance 1. This point can have a big impact on the shape of the best-fitting line. Depending on your data, it might be necessary to remove outliers prior to training.\n",
    "\n",
    "![](images/outlier.jpg)<br/> Outliers with linear regression: http://www.unige.ch/ses/sococ/cl///stat/illust/nonlin.html\n",
    "\n",
    "- We now used the raw measurements as input to our model even though the different features all have different scales. In principle the scale does not matter to the linear regression algorithm but it does make it hard to interpret the found coefficients. Does a large coefficient indicate an important value or is it just compensating for a very small input feature? It is often a good idea to **normalize** your data first. There are two common techniques to do this:\n",
    "    - **Z-scores**: Subtract the mean, divide by the standard deviation $\\frac{x - \\mu}{\\sigma}$ (also called **Standardization**)\n",
    "    - **Normalize**: $ \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "- For very large input numbers it might be interesting to use $\\sqrt{X}, log(x)$ or some other preprocessing of the values.\n",
    "- One of the first steps was to calculate the correlation between all our features. It is often a bad idea to train the model on two features that are highly correlated with each other. Since they contain the same information, they will not help the model. Surprisingly they may even harm the training of the model as they might result in numerical problems during training: https://towardsdatascience.com/why-exclude-highly-correlated-features-when-building-regression-model-34d77a90ea8e\n",
    "- In our dataset all features were represented with continuous values. Some features however can only assume discrete values (e.g. a feature that indicates which neighborhood a house is located). The most common technique to encode these types of features is to use a **one-hot** encoding.\n",
    "- A problem with the $R^2$ metric is that there is no penalty for adding addition input features. A model with more input features might result in a higher $R2$ value just by chance. To deal with this, you should use the **adjusted $R^2$**: https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2\n",
    "\n",
    "![](images/curve_fitting.png)<br/> XKCD: Curve fitting: https://xkcd.com/2048/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
